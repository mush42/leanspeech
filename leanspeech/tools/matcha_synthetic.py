import argparse
import functools
import itertools
import json
import os
import random
import sys
from pathlib import Path

import numpy as np
import torch
import onnxruntime
import rootutils
from lightning import LightningModule
from more_itertools import chunked
from tqdm import tqdm

from leanspeech.text import process_and_phonemize_text_matcha
from leanspeech.utils import get_script_logger, normalize_mel, numpy_pad_sequences, numpy_unpad_sequences


log = get_script_logger(__name__)


ONNX_CUDA_PROVIDERS = [
    ("CUDAExecutionProvider", {"cudnn_conv_algo_search": "DEFAULT"}),
    "CPUExecutionProvider"
]
ONNX_CPU_PROVIDERS = ["CPUExecutionProvider",]


class MatchaInferenceWrapper(LightningModule):
    def __init__(self, matcha):
        super().__init__()
        self.matcha = matcha

    @torch.inference_mode()
    def synthesise(self, x, x_lengths, n_timesteps, temperature=1.0, spks=None, length_scale=1.0):
        """
        Generates mel-spectrogram from text. Returns:
            1. encoder outputs
            2. decoder outputs
            3. generated alignment

        Args:
            x (torch.Tensor): batch of texts, converted to a tensor with phoneme embedding ids.
                shape: (batch_size, max_text_length)
            x_lengths (torch.Tensor): lengths of texts in batch.
                shape: (batch_size,)
            n_timesteps (int): number of steps to use for reverse diffusion in decoder.
            temperature (float, optional): controls variance of terminal distribution.
            spks (bool, optional): speaker ids.
                shape: (batch_size,)
            length_scale (float, optional): controls speech pace.
                Increase value to slow down generated speech and vice versa.

        Returns:
            dict: {
                "encoder_outputs": torch.Tensor, shape: (batch_size, n_feats, max_mel_length),
                # Average mel spectrogram generated by the encoder
                "decoder_outputs": torch.Tensor, shape: (batch_size, n_feats, max_mel_length),
                # Refined mel spectrogram improved by the CFM
                "attn": torch.Tensor, shape: (batch_size, max_text_length, max_mel_length),
                # Alignment map between text and mel spectrogram
                "mel": torch.Tensor, shape: (batch_size, n_feats, max_mel_length),
                # Denormalized mel spectrogram
                "mel_lengths": torch.Tensor, shape: (batch_size,),
                # Lengths of mel spectrograms
                "w_ceil": torch.Tensor, shape: (batch_size, max_text_length),
                # durations of phonemes in mel frames
        """
        from matcha.utils.model import  denormalize, fix_len_compatibility, generate_path, sequence_mask

        x = x.to(self.device)
        x_lengths = x_lengths.to("cpu")

        if self.matcha.n_spks > 1:
            # Get speaker embedding
            spks = self.matcha.spk_emb(spks.long())

        # Get encoder_outputs `mu_x` and log-scaled token durations `logw`
        mu_x, logw, x_mask = self.matcha.encoder(x, x_lengths, spks)
        mu_x = mu_x.to(self.device)
        logw = logw.to(self.device)
        x_mask = x_mask.to(self.device)

        w = torch.exp(logw) * x_mask
        w_ceil = torch.ceil(w) * length_scale
        y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()
        y_max_length = y_lengths.max()
        y_max_length_ = fix_len_compatibility(y_max_length)

        # Using obtained durations `w` construct alignment map `attn`
        y_mask = sequence_mask(y_lengths, y_max_length_).unsqueeze(1).to(x_mask.dtype)
        attn_mask = x_mask.unsqueeze(-1) * y_mask.unsqueeze(2)
        attn = generate_path(w_ceil.squeeze(1), attn_mask.squeeze(1)).unsqueeze(1)
        attn = attn.to(self.device)

        # Align encoded text and get mu_y
        mu_y = torch.matmul(attn.squeeze(1).transpose(1, 2), mu_x.transpose(1, 2))
        mu_y = mu_y.transpose(1, 2)

        # Generate sample tracing the probability flow
        decoder_outputs = self.matcha.decoder(mu_y, y_mask, n_timesteps, temperature, spks)

        return {
            "attn": attn[:, :, :y_max_length],
            "mel": denormalize(decoder_outputs, self.matcha.mel_mean, self.matcha.mel_std),
            "mel_lengths": y_lengths,
            "w_ceil": w_ceil
        }


def infer_onnx(model, inputs):
    mel_specs, mel_lengths, w_ceil = model.run(None, inputs)
    return mel_specs, mel_lengths, w_ceil


def infer_torch(model, inputs, n_timesteps=10):
    x = torch.from_numpy(inputs["x"])
    x_lengths = torch.from_numpy(inputs["x_lengths"])
    outputs = model.synthesise(x, x_lengths, n_timesteps)
    return (
        outputs["mel"].detach().cpu().numpy(),
        outputs["mel_lengths"].detach().cpu().numpy(),
        outputs["w_ceil"].detach().cpu().numpy(),
    )


def main():
    root_path = rootutils.find_root(search_from=__file__, indicator=".project-root")

    parser = argparse.ArgumentParser()

    parser.add_argument("model_path", type=str, help="Exported matcha checkpoint.")
    parser.add_argument("language", type=str, help="Language to use for phonemization.")
    parser.add_argument("text_file", type=str, help="Text file containing lines to synthesize.")
    parser.add_argument("output_directory", type=str, help="Directory to write files to.")
    parser.add_argument("--cuda", action="store_true", help="Use GPU for onnx inference.")
    parser.add_argument(
        "--batch-size", type=int, default=32, help="Number of sentences to synthesize in one run."
    )
    parser.add_argument(
        "--mel-mean", type=float, default=0.0, help="dataset-specific mel mean"
    )
    parser.add_argument(
        "--mel-std", type=float, default=1.0, help="dataset-specific mel std"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.667, help="Matcha temperature."
    )
    parser.add_argument(
        "--length-scale", type=float, default=1.0, help="Matcha temperature."
    )
    parser.add_argument(
        "--val-percent", type=float, default=0.05, help="Percentage of sentences for validation"
    )

    args = parser.parse_args()

    sents = Path(args.text_file).read_text(encoding="utf-8").splitlines()
    sents = [s.strip() for s in sents if s.strip()]

    log.info(f"Found {len(sents)} sentences")

    model_path = Path(args.model_path)
    if model_path.suffix == ".ckpt":
        from matcha.cli import load_matcha
        log.info(f"Loading matcha checkpoint from: {args.model_path}")
        matcha_torch = load_matcha(model_path.stem, model_path, "cpu")
        model = MatchaInferenceWrapper(matcha_torch)
        if args.cuda:
            model.to("cuda")
        model.eval()
        infer_func = functools.partial(infer_torch, model)
    elif model_path.suffix == ".onnx":
        log.info(f"Loading matcha onnx from: {args.model_path}")
        matcha_onnx = onnxruntime.InferenceSession(
            args.model_path,
            providers=ONNX_CUDA_PROVIDERS if args.cuda else ONNX_CPU_PROVIDERS
        )
        infer_func = functools.partial(infer_onnx, matcha_onnx)
    else:
        log.error(f"Invalid model: `{args.model_path}`")
        sys.exit(-1)

    scales = np.array([args.temperature, args.length_scale], dtype=np.float32)

    total_mel_len = 0
    batch_iterator = tqdm(
        enumerate(chunked(sents, args.batch_size)),
        total=len(sents) // args.batch_size,
        desc="synthesizing",
        unit="batch"
    )
    output_dir = Path(args.output_directory)
    output_dir.mkdir(parents=True, exist_ok=True)
    data_output_dir = output_dir.joinpath("data")
    data_output_dir.mkdir(parents=True, exist_ok=True)
    filelist = []
    item_counter = itertools.count()
    for (idx, batch) in batch_iterator:
        phoneme_ids = []
        lengths = []
        texts = []
        jsons = []
        for sent in batch:
            phids, text = process_and_phonemize_text_matcha(sent, args.language)
            phoneme_ids.append(phids)
            lengths.append(len(phids))
            texts .append(text)
            json_data = {"phoneme_ids": phids, "text": text}
            jsons.append(json_data)

        x = numpy_pad_sequences(phoneme_ids).astype(np.int64)
        x_lengths = np.array(lengths, dtype=np.int64)
        matcha_inputs = {
            "x": x,
            "x_lengths": x_lengths,
            "scales": scales
        }
        mel_specs, mel_lengths, w_ceil = infer_func(matcha_inputs)
        mels = numpy_unpad_sequences(mel_specs, mel_lengths)
        durs = numpy_unpad_sequences(w_ceil, x_lengths)
        total_mel_len += mel_lengths.sum().item()

        for (j, m, d) in zip(jsons, mels, durs):
            item_number = next(item_counter)
            file_stem = "gen-" + str(item_number).rjust(5, "0")
            filelist.append(file_stem)
            outfile = data_output_dir.joinpath(file_stem)
            out_json = outfile.with_suffix(".json")
            out_mel = outfile.with_suffix(".mel.npy")
            out_dur = outfile.with_suffix(".dur.npy")
            with open(out_json, "w", encoding="utf-8") as file:
                json.dump(j, file, ensure_ascii=False)
            mel = normalize_mel(m, args.mel_mean, args.mel_std)
            np.save(out_mel, mel, allow_pickle=False)
            np.save(out_dur, d.squeeze(), allow_pickle=False)

    log.info(f"Total mel lengths: {total_mel_len} frames")

    random.shuffle(filelist)
    val_limit = int(len(filelist) * args.val_percent)
    train_split = filelist[val_limit:]
    val_split = filelist[:val_limit]
    with open(output_dir.joinpath("filelist.txt"), "w", encoding="utf-8") as file:
        file.write("\n".join(filelist))
        log.info(f"Wrote filelist to `filelist.txt`")

    with open(output_dir.joinpath("train.txt"), "w", encoding="utf-8") as file:
        train_filepaths = [
            os.fspath(
                data_output_dir.joinpath(fname).absolute()
            )
            for fname in train_split
        ]
        file.write("\n".join(train_filepaths))
        log.info(f"Wrote file paths to `train.txt`")

    with open(output_dir.joinpath("val.txt"), "w", encoding="utf-8") as file:
        val_filepaths = [
            os.fspath(
                data_output_dir.joinpath(fname).absolute()
            )
            for fname in val_split
        ]
        file.write("\n".join(val_filepaths))
        log.info(f"Wrote file paths to `val.txt`")

    log.info(f"Wrote dataset to {args.output_directory}")


if __name__ == '__main__':
    main()
