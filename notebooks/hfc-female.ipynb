{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeanSpeech Training: HFC-Female (en-US)\n",
    "This notebook allows you to train [LeanSpeech TTS](https://github.com/mush42/leanspeech) on [HiFiCaptin en-US female dataset](https://ast-astrec.nict.go.jp/en/release/hi-fi-captain/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plumming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@markdown ### Google Colab Anti-Disconnect\n",
    "#@markdown Avoid automatic disconnection. Still, it will disconnect after **6 to 12 hours**.\n",
    "\n",
    "import IPython\n",
    "js_code = '''\n",
    "function ClickConnect(){\n",
    "console.log(\"Working\");\n",
    "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "'''\n",
    "display(IPython.display.Javascript(js_code))\n",
    "\n",
    "\n",
    "#@markdown ### Check GPU type\n",
    "#@markdown A higher capable GPU can lead to faster training speeds. By default, you will have a **Tesla T4**.\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#@markdown ### Clone leanSpeech repository\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(os.path.join(os.getcwd(), \"leanspeech\")):\n",
    "    print(\"Cloning leanspeech repository...\")\n",
    "    !git clone --depth=1 https://github.com/mush42/leanspeech\n",
    "\n",
    "!cd ./leanSpeech\n",
    "\n",
    "#@markdown ### Upgrade packages\n",
    "!pip install --upgrade pip setuptools wheel\n",
    "\n",
    "#@markdown ### Install leanspeech dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "HFC_FEMALE_DATASET_URL = \"https://ast-astrec.nict.go.jp/release/hi-fi-captain/hfc_en-US_F.zip\"\n",
    "HFC_FEMALE_MATCHA_ONNX = \"https://drive.google.com/file/d/18ysTCnXipCzzK6LxJtXFJvpDa6QKt7Ol/view?usp=sharing\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload sentences to generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "(filename, file_content) = tuple(uploaded.items())\n",
    "print(f\"Uploaded file: {filename}\")\n",
    "source_text = file_content.decode(\"utf-8\")\n",
    "source_lines = source_text.splitlines()\n",
    "source_lines = [line for l in source_lines if (line := l.strip())]\n",
    "print(f\"Found {len(source_lines)} lines in the uploaded file\")\n",
    "with open(\"sentences.txt\", \"w\", encoding=\"utf-8\") as outfile:\n",
    "    outfile.write(\"\\n\".join(source_lines ))\n",
    "print(\"Wrote lines to `sentences.txt`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m leanspeech.tools.matcha_synthetic \\\n",
    "    --batch-size 64 \\\n",
    "    --mel-mean \"-6.38385\" \\\n",
    "    --mel-std 2.541796 \\\n",
    "    --val-percent 0.025 \\\n",
    "    --cuda \\\n",
    "    matcha-hfc_female.onnx \\\n",
    "    en-us \\\n",
    "    sentences.txt \\\n",
    "    ./data/synthetic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format dataset as ljspeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Downloading dataset...\")\n",
    "!wget \"https://ast-astrec.nict.go.jp/release/hi-fi-captain/hfc_en-US_F.zip\"\n",
    "\n",
    "print(\"Formatting dataset as ljspeech...\")\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path, PurePosixPath\n",
    "from zipfile import ZipFile\n",
    "\n",
    "raw_data_output_dir = \"./data/raw/\"\n",
    "\n",
    "train_output_dir = raw_data_output_dir .joinpath(\"train\")\n",
    "val_output_dir = raw_data_output_dir .joinpath(\"val\")\n",
    "\n",
    "wav_train_output_dir = train_output_dir.joinpath(\"wav\")\n",
    "wav_val_output_dir = val_output_dir.joinpath(\"wav\")\n",
    "\n",
    "wav_train_output_dir .mkdir(parents=True, exist_ok=True)\n",
    "wav_val_output_dir .mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wav2sent_train = {}\n",
    "wav2sent_val = {}\n",
    "\n",
    "print(\"Copying wav files...\")\n",
    "with open(\"hfc_en-US_F.zip\", \"r\") as zfile:\n",
    "    for fp in zfile.filelist:\n",
    "        p = PurePosixPath(fp)\n",
    "        if p.suffix != \".txt\":\n",
    "            continue\n",
    "        content = zfile.read(fp).decode(\"utf-8\")\n",
    "        lines = [l for l in content.splitlines() if l.strip()]\n",
    "        w2s = wav2sent_train if \"val\" not in p.name else wav2sent_val\n",
    "        for line in lines:\n",
    "            filestem, sent = line.split(\" \", 1)\n",
    "            w2s [filestem] = sent\n",
    "\n",
    "    for fp in zfile.filelist:\n",
    "        p = PurePosixPath(fp)\n",
    "        if p.suffix != \".wav\":\n",
    "            continue\n",
    "        if p.stem in wav2sent_train:\n",
    "            output_path = os.fspath(wav_train_output_dir  / p.name)\n",
    "        elif p.stem in wav2sent_val:\n",
    "            output_path = os.fspath(wav_val_output_dir  / p.name)\n",
    "        else:\n",
    "            print(f\"Warning: file `{fp} not found in train/val list.\")\n",
    "            continue\n",
    "        zfile.extract(fp, path=output_path)\n",
    "\n",
    "print(\"Writing metadata.csv\")\n",
    "with open(train_output_dir .joinpath(\"metadata.csv\"), \"w\", encoding=\"utf-8\") as cfile:\n",
    "    writer = csv.writer(cfile, delimiter=\"|\")\n",
    "    writer.writerows(tuple(wav2sent_train.items()))\n",
    "\n",
    "with open(val_output_dir .joinpath(\"metadata.csv\"), \"w\", encoding=\"utf-8\") as cfile:\n",
    "    writer = csv.writer(cfile, delimiter=\"|\")\n",
    "    writer.writerows(tuple(wav2sent_val.items()))\n",
    "\n",
    "print(\"Done formatting dataset as `ljspeech`\")\n",
    "\n",
    "!ls ./data/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess ground-truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python3 -m leanspeech.tools.preprocess_dataset hfc_female-en-US ./data/raw ./data/gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!python3 leanspeech.train experiment=\"hfc_female-en_US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_synth_lines = Path(\"./data/synthetic/train.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "val_synth_lines = Path(\"./data/synthetic/val.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "train_gt_lines = Path(\"./data/gt/train.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "val_gt_lines = Path(\"./data/gt/val.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "all_train_lines = train_synth_lines + train_gt_lines\n",
    "all_val_lines = val_synth_lines + val_gt_lines\n",
    "\n",
    "random.shuffle(all_train_lines)\n",
    "random.shuffle(all_val_lines)\n",
    "\n",
    "with open(\"./data/train.txt\", \"w\", encoding=\"utf-8\") as tfile:\n",
    "    tfile.write(\"\\n\".join(all_train_lines))\n",
    "\n",
    "with open(\"./data/val.txt\", \"w\", encoding=\"utf-8\") as vfile:\n",
    "    vfile.write(\"\\n\".join(all_val_lines))\n",
    "\n",
    "# Move files to expected location\n",
    "!mkdir data/hfc_female-en_US\n",
    "!mv data/train.txt data/hfc_female-en_US\n",
    "!mv data/val.txt data/hfc_female-en_US\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
